{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from gensim import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Convolution2D, Conv2D, MaxPooling2D, Lambda, GlobalMaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Activation, AveragePooling2D, Concatenate\n",
    "from keras.layers import Dense, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D, Embedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Analysis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Some very good simulations of bus v car can b...</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.910</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the all time greatest price and rates ever...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.900</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Your services at Bengaluru Airport is so scre...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.700</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lots of good analysis on Uber IPO in  the shar...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.675</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Seeing this I didnt book any ride from you  w...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.600</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  \\\n",
       "0           0   Some very good simulations of bus v car can b...   \n",
       "1           1      the all time greatest price and rates ever...   \n",
       "2           2   Your services at Bengaluru Airport is so scre...   \n",
       "3           3  Lots of good analysis on Uber IPO in  the shar...   \n",
       "4           4   Seeing this I didnt book any ride from you  w...   \n",
       "\n",
       "   Subjectivity  Polarity  Analysis  label  \n",
       "0         0.780     0.910  Positive      1  \n",
       "1         0.875     0.900  Positive      1  \n",
       "2         0.600     0.700  Positive      1  \n",
       "3         0.625     0.675  Positive      1  \n",
       "4         0.700     0.600  Positive      1  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Uber_Dataset1.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Subjectivity',axis=1,inplace=True)\n",
    "data.drop('Polarity',axis=1,inplace=True)\n",
    "data.drop('Analysis',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Some very good simulations of bus v car can b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the all time greatest price and rates ever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Your services at Bengaluru Airport is so scre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lots of good analysis on Uber IPO in  the shar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Seeing this I didnt book any ride from you  w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  label\n",
       "0           0   Some very good simulations of bus v car can b...      1\n",
       "1           1      the all time greatest price and rates ever...      1\n",
       "2           2   Your services at Bengaluru Airport is so scre...      1\n",
       "3           3  Lots of good analysis on Uber IPO in  the shar...      1\n",
       "4           4   Seeing this I didnt book any ride from you  w...      1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def remove_punct(text):\n",
    "    text_nopunct = ''\n",
    "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
    "    return text_nopunct\n",
    "data['Text_Clean'] = data['tweet'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = [word_tokenize(sen) for sen in data.Text_Clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_token(tokens): \n",
    "    return [w.lower() for w in tokens]    \n",
    "    \n",
    "lower_tokens = [lower_token(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "def removeStopWords(tokens): \n",
    "    return [word for word in tokens if word not in stoplist]\n",
    "filtered_words = [removeStopWords(sen) for sen in lower_tokens]\n",
    "data['Text_Final'] = [' '.join(sen) for sen in filtered_words]\n",
    "data['tokens'] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_Final</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good simulations bus v car done uber pool even...</td>\n",
       "      <td>[good, simulations, bus, v, car, done, uber, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time greatest price rates ever paid company gr...</td>\n",
       "      <td>[time, greatest, price, rates, ever, paid, com...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>services bengaluru airport screwed cab drivers...</td>\n",
       "      <td>[services, bengaluru, airport, screwed, cab, d...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lots good analysis uber ipo share price determ...</td>\n",
       "      <td>[lots, good, analysis, uber, ipo, share, price...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seeing didnt book ride went served much better...</td>\n",
       "      <td>[seeing, didnt, book, ride, went, served, much...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Text_Final  \\\n",
       "0  good simulations bus v car done uber pool even...   \n",
       "1  time greatest price rates ever paid company gr...   \n",
       "2  services bengaluru airport screwed cab drivers...   \n",
       "3  lots good analysis uber ipo share price determ...   \n",
       "4  seeing didnt book ride went served much better...   \n",
       "\n",
       "                                              tokens  label  Pos  Neg  \n",
       "0  [good, simulations, bus, v, car, done, uber, p...      1    1    0  \n",
       "1  [time, greatest, price, rates, ever, paid, com...      1    1    0  \n",
       "2  [services, bengaluru, airport, screwed, cab, d...      1    1    0  \n",
       "3  [lots, good, analysis, uber, ipo, share, price...      1    1    0  \n",
       "4  [seeing, didnt, book, ride, went, served, much...      1    1    0  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for l in data.label:\n",
    "    if l == 0:\n",
    "        pos.append(0)\n",
    "        neg.append(1)\n",
    "    elif l == 1:\n",
    "        pos.append(1)\n",
    "        neg.append(0)\n",
    "data['Pos']= pos\n",
    "data['Neg']= neg\n",
    "\n",
    "data = data[['Text_Final', 'tokens', 'label', 'Pos', 'Neg']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, \n",
    "                                         test_size=0.3, \n",
    "                                         random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2207 words total, with a vocabulary size of 1117\n",
      "Max sentence length is 36\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880 words total, with a vocabulary size of 557\n",
      "Max sentence length is 33\n"
     ]
    }
   ],
   "source": [
    "all_test_words = [word for tokens in data_test[\"tokens\"] for word in tokens]\n",
    "test_sentence_lengths = [len(tokens) for tokens in data_test['tokens']]\n",
    "TEST_VOCAB = sorted(list(set(all_test_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_test_words), len(TEST_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(test_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = 'C:/Users/hp/Sentiment-Analysis-RNN-LSTM/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1117 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(data_train['Text_Final'].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(data_train['Text_Final'].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "train_cnn_data = pad_sequences(training_sequences, \n",
    "                               maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1118, 300)\n"
     ]
    }
   ],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
    " EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, labels_index):\n",
    " \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=False)\n",
    "    \n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4,5,6]\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=10, \n",
    "                        kernel_size=filter_size, \n",
    "                        activation='relu')(embedded_sequences)\n",
    "        l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "        convs.append(l_pool)\n",
    "    l_merge = concatenate(convs, axis=1)\n",
    "    x = Dropout(0.1)(l_merge)  \n",
    "    x = Dense(75, activation='relu')(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['Pos', 'Neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 50, 300)      335400      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 49, 10)       6010        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 48, 10)       9010        embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 47, 10)       12010       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 46, 10)       15010       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 45, 10)       18010       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_25 (Global (None, 10)           0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_26 (Global (None, 10)           0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_27 (Global (None, 10)           0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_28 (Global (None, 10)           0           conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_29 (Global (None, 10)           0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 50)           0           global_max_pooling1d_25[0][0]    \n",
      "                                                                 global_max_pooling1d_26[0][0]    \n",
      "                                                                 global_max_pooling1d_27[0][0]    \n",
      "                                                                 global_max_pooling1d_28[0][0]    \n",
      "                                                                 global_max_pooling1d_29[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50)           0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 75)           3825        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 2)            152         dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 399,427\n",
      "Trainable params: 64,027\n",
      "Non-trainable params: 335,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, \n",
    "                MAX_SEQUENCE_LENGTH, \n",
    "                len(train_word_index)+1, \n",
    "                EMBEDDING_DIM, \n",
    "                len(list(label_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train[label_names].values\n",
    "\n",
    "x_train = train_cnn_data\n",
    "y_tr = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 75ms/step - loss: 0.6731 - acc: 0.6569 - val_loss: 0.6639 - val_acc: 0.5833\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 0.5864 - acc: 0.6569 - val_loss: 0.6617 - val_acc: 0.5833\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.5564 - acc: 0.7353 - val_loss: 0.6587 - val_acc: 0.5833\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.4682 - acc: 0.7549 - val_loss: 0.7018 - val_acc: 0.5833\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 0.3985 - acc: 0.8529 - val_loss: 0.7014 - val_acc: 0.5833\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 10\n",
    "hist = model.fit(x_train, \n",
    "                 y_tr, \n",
    "                 epochs=num_epochs, \n",
    "                 validation_split=0.1, \n",
    "                 shuffle=True, \n",
    "                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(data_test[\"Text_Final\"].tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6326530612244898"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_cnn_data, \n",
    "                            batch_size=3, \n",
    "                            verbose=1)\n",
    "labels = [1, 0]\n",
    "prediction_labels=[]\n",
    "for p in predictions:\n",
    "    prediction_labels.append(labels[np.argmax(p)])\n",
    "sum(data_test.label==prediction_labels)/len(prediction_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
